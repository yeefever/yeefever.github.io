This project focuses on fine-tuning a text-to-speech (TTS) model for a single emotion, addressing two key research questions: (1) the effectiveness of smaller, resource-efficient TTS models in capturing and synthesizing emotional prosody, and (2) their ability to generalize emotional speech synthesis in low-resource settings, particularly with limited training data and diverse speaker profiles and emotional expressions.